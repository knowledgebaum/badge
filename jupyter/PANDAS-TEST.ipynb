{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>symptom</th>\n",
       "      <th>intensity</th>\n",
       "      <th>duration</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>trigger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-13 04:30:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-12 08:45:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-12</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-11 15:55:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-11</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-11 02:20:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>2019-04-11</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-10 02:45:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-04-09 07:05:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-04-09 00:35:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-04-08 05:45:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-04-07 16:20:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-04-06 15:55:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-06</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-04-05 15:50:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-04-05 06:50:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-04-04 07:40:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-04-03 05:00:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-04-01 06:25:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-03-31 05:00:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-03-31 05:00:00</td>\n",
       "      <td>bloating</td>\n",
       "      <td>90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-03-30 05:30:00</td>\n",
       "      <td>bloating</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-03-30 05:30:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-03-29 06:20:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>60.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-03-28 04:40:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-03-24 06:20:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-03-24 19:00:00</td>\n",
       "      <td>bloating</td>\n",
       "      <td>70.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-03-24 19:00:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>85.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-03-24 17:25:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-03-23 06:00:00</td>\n",
       "      <td>bloating</td>\n",
       "      <td>65.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-03-23 06:00:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>85.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-03-21 02:00:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-03-20 06:30:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-03-20 04:50:00</td>\n",
       "      <td>gas</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>2019-03-16 06:35:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Metamucil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>2019-03-16 02:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Metamucil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>2019-03-15 22:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Metamucil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>2019-03-15 16:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Metamucil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>2019-03-14 07:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Metamucil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>2019-03-31 08:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>2019-03-23 00:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>2019-03-18 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>2019-04-10 04:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>2019-04-10 04:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>2019-04-09 05:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>2019-04-09 05:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>2019-04-08 03:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>runnning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>2019-04-07 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2019-04-07 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2019-04-05 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>2019-04-05 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>2019-04-02 05:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>2019-04-02 05:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>2019-03-30 21:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>hiking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2019-03-29 02:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2019-03-29 02:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2019-03-23 21:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2019-03-22 03:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2019-03-22 03:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>2019-03-20 03:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>2019-03-20 03:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>weightlifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>2019-03-17 23:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>2019-03-16 18:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>2019-04-08 23:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-09</td>\n",
       "      <td>2019-04-07</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>piroxicam olamine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ts   symptom  intensity  duration        day  \\\n",
       "0    2019-04-13 04:30:00       gas       35.0       0.0 2019-04-13   \n",
       "1    2019-04-12 08:45:00       gas       10.0       0.0 2019-04-12   \n",
       "2    2019-04-11 15:55:00       gas       60.0       0.0 2019-04-11   \n",
       "3    2019-04-11 02:20:00       gas       75.0     360.0 2019-04-11   \n",
       "4    2019-04-10 02:45:00       gas       75.0      60.0 2019-04-10   \n",
       "5    2019-04-09 07:05:00       gas       25.0       0.0 2019-04-09   \n",
       "6    2019-04-09 00:35:00       gas       40.0       0.0 2019-04-09   \n",
       "7    2019-04-08 05:45:00       gas       25.0       0.0 2019-04-08   \n",
       "8    2019-04-07 16:20:00       gas       60.0       0.0 2019-04-07   \n",
       "9    2019-04-06 15:55:00       gas       35.0       0.0 2019-04-06   \n",
       "10   2019-04-05 15:50:00       gas       75.0       0.0 2019-04-05   \n",
       "11   2019-04-05 06:50:00       gas       75.0       0.0 2019-04-05   \n",
       "12   2019-04-04 07:40:00       gas       25.0       0.0 2019-04-04   \n",
       "13   2019-04-03 05:00:00       gas       60.0       0.0 2019-04-03   \n",
       "14   2019-04-01 06:25:00       gas       50.0       0.0 2019-04-01   \n",
       "15   2019-03-31 05:00:00       gas       90.0     180.0 2019-03-31   \n",
       "16   2019-03-31 05:00:00  bloating       90.0     180.0 2019-03-31   \n",
       "17   2019-03-30 05:30:00  bloating       65.0       0.0 2019-03-30   \n",
       "18   2019-03-30 05:30:00       gas       75.0       0.0 2019-03-30   \n",
       "19   2019-03-29 06:20:00       gas       60.0     280.0 2019-03-29   \n",
       "20   2019-03-28 04:40:00       gas       40.0       0.0 2019-03-28   \n",
       "21   2019-03-24 06:20:00       gas       75.0       0.0 2019-03-24   \n",
       "22   2019-03-24 19:00:00  bloating       70.0     165.0 2019-03-25   \n",
       "23   2019-03-24 19:00:00       gas       85.0     165.0 2019-03-25   \n",
       "24   2019-03-24 17:25:00       gas       15.0       0.0 2019-03-25   \n",
       "25   2019-03-23 06:00:00  bloating       65.0     270.0 2019-03-23   \n",
       "26   2019-03-23 06:00:00       gas       85.0     270.0 2019-03-23   \n",
       "27   2019-03-21 02:00:00       gas       30.0       0.0 2019-03-21   \n",
       "28   2019-03-20 06:30:00       gas       60.0       0.0 2019-03-20   \n",
       "29   2019-03-20 04:50:00       gas       30.0       0.0 2019-03-20   \n",
       "..                   ...       ...        ...       ...        ...   \n",
       "575  2019-03-16 06:35:00       NaN        NaN       NaN 2019-03-16   \n",
       "576  2019-03-16 02:30:00       NaN        NaN       NaN 2019-03-16   \n",
       "577  2019-03-15 22:15:00       NaN        NaN       NaN 2019-03-16   \n",
       "578  2019-03-15 16:15:00       NaN        NaN       NaN 2019-03-15   \n",
       "579  2019-03-14 07:10:00       NaN        NaN       NaN 2019-03-14   \n",
       "580  2019-03-31 08:45:00       NaN        NaN       NaN 2019-03-31   \n",
       "581  2019-03-23 00:20:00       NaN        NaN       NaN 2019-03-23   \n",
       "582  2019-03-18 06:00:00       NaN        NaN       NaN 2019-03-18   \n",
       "583  2019-04-10 04:10:00       NaN        NaN       NaN 2019-04-10   \n",
       "584  2019-04-10 04:10:00       NaN        NaN       NaN 2019-04-10   \n",
       "585  2019-04-09 05:10:00       NaN        NaN       NaN 2019-04-09   \n",
       "586  2019-04-09 05:10:00       NaN        NaN       NaN 2019-04-09   \n",
       "587  2019-04-08 03:45:00       NaN        NaN       NaN 2019-04-08   \n",
       "588  2019-04-07 02:00:00       NaN        NaN       NaN 2019-04-07   \n",
       "589  2019-04-07 02:00:00       NaN        NaN       NaN 2019-04-07   \n",
       "590  2019-04-05 02:00:00       NaN        NaN       NaN 2019-04-05   \n",
       "591  2019-04-05 02:00:00       NaN        NaN       NaN 2019-04-05   \n",
       "592  2019-04-02 05:30:00       NaN        NaN       NaN 2019-04-02   \n",
       "593  2019-04-02 05:30:00       NaN        NaN       NaN 2019-04-02   \n",
       "594  2019-03-30 21:30:00       NaN        NaN       NaN 2019-03-31   \n",
       "595  2019-03-29 02:45:00       NaN        NaN       NaN 2019-03-29   \n",
       "596  2019-03-29 02:45:00       NaN        NaN       NaN 2019-03-29   \n",
       "597  2019-03-23 21:45:00       NaN        NaN       NaN 2019-03-24   \n",
       "598  2019-03-22 03:30:00       NaN        NaN       NaN 2019-03-22   \n",
       "599  2019-03-22 03:30:00       NaN        NaN       NaN 2019-03-22   \n",
       "600  2019-03-20 03:30:00       NaN        NaN       NaN 2019-03-20   \n",
       "601  2019-03-20 03:30:00       NaN        NaN       NaN 2019-03-20   \n",
       "602  2019-03-17 23:10:00       NaN        NaN       NaN 2019-03-18   \n",
       "603  2019-03-16 18:30:00       NaN        NaN       NaN 2019-03-17   \n",
       "604  2019-04-08 23:20:00       NaN        NaN       NaN 2019-04-09   \n",
       "\n",
       "           week       month            trigger  \n",
       "0    2019-04-07  2019-04-01                NaN  \n",
       "1    2019-04-07  2019-04-01                NaN  \n",
       "2    2019-04-07  2019-04-01                NaN  \n",
       "3    2019-04-07  2019-04-01                NaN  \n",
       "4    2019-04-07  2019-04-01                NaN  \n",
       "5    2019-04-07  2019-04-01                NaN  \n",
       "6    2019-04-07  2019-04-01                NaN  \n",
       "7    2019-04-07  2019-04-01                NaN  \n",
       "8    2019-04-07  2019-04-01                NaN  \n",
       "9    2019-03-31  2019-04-01                NaN  \n",
       "10   2019-03-31  2019-04-01                NaN  \n",
       "11   2019-03-31  2019-04-01                NaN  \n",
       "12   2019-03-31  2019-04-01                NaN  \n",
       "13   2019-03-31  2019-04-01                NaN  \n",
       "14   2019-03-31  2019-04-01                NaN  \n",
       "15   2019-03-31  2019-03-01                NaN  \n",
       "16   2019-03-31  2019-03-01                NaN  \n",
       "17   2019-03-24  2019-03-01                NaN  \n",
       "18   2019-03-24  2019-03-01                NaN  \n",
       "19   2019-03-24  2019-03-01                NaN  \n",
       "20   2019-03-24  2019-03-01                NaN  \n",
       "21   2019-03-24  2019-03-01                NaN  \n",
       "22   2019-03-24  2019-03-01                NaN  \n",
       "23   2019-03-24  2019-03-01                NaN  \n",
       "24   2019-03-24  2019-03-01                NaN  \n",
       "25   2019-03-17  2019-03-01                NaN  \n",
       "26   2019-03-17  2019-03-01                NaN  \n",
       "27   2019-03-17  2019-03-01                NaN  \n",
       "28   2019-03-17  2019-03-01                NaN  \n",
       "29   2019-03-17  2019-03-01                NaN  \n",
       "..          ...         ...                ...  \n",
       "575  2019-03-10  2019-03-01          Metamucil  \n",
       "576  2019-03-10  2019-03-01          Metamucil  \n",
       "577  2019-03-10  2019-03-01          Metamucil  \n",
       "578  2019-03-10  2019-03-01          Metamucil  \n",
       "579  2019-03-10  2019-03-01          Metamucil  \n",
       "580  2019-03-31  2019-03-01          happiness  \n",
       "581  2019-03-17  2019-03-01              angry  \n",
       "582  2019-03-17  2019-03-01               Love  \n",
       "583  2019-04-07  2019-04-01             biking  \n",
       "584  2019-04-07  2019-04-01      weightlifting  \n",
       "585  2019-04-07  2019-04-01      weightlifting  \n",
       "586  2019-04-07  2019-04-01             biking  \n",
       "587  2019-04-07  2019-04-01           runnning  \n",
       "588  2019-04-07  2019-04-01             biking  \n",
       "589  2019-04-07  2019-04-01      weightlifting  \n",
       "590  2019-03-31  2019-04-01             biking  \n",
       "591  2019-03-31  2019-04-01      weightlifting  \n",
       "592  2019-03-31  2019-04-01      weightlifting  \n",
       "593  2019-03-31  2019-04-01             biking  \n",
       "594  2019-03-31  2019-03-01             hiking  \n",
       "595  2019-03-24  2019-03-01      weightlifting  \n",
       "596  2019-03-24  2019-03-01             biking  \n",
       "597  2019-03-24  2019-03-01            walking  \n",
       "598  2019-03-17  2019-03-01             biking  \n",
       "599  2019-03-17  2019-03-01      weightlifting  \n",
       "600  2019-03-17  2019-03-01             biking  \n",
       "601  2019-03-17  2019-03-01      weightlifting  \n",
       "602  2019-03-17  2019-03-01            walking  \n",
       "603  2019-03-17  2019-03-01            walking  \n",
       "604  2019-04-07  2019-04-01  piroxicam olamine  \n",
       "\n",
       "[940 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#LOAD DATA\n",
    "\n",
    "sympPath =  Path('C:/webDev/pycharm/dieta/data/ak_symptoms.csv')\n",
    "trigPath =  Path('C:/webDev/pycharm/dieta/data/ak_triggers.csv')\n",
    "\n",
    "\n",
    "symp_df = pd.read_csv(sympPath)\n",
    "trig_df = pd.read_csv(trigPath)\n",
    "symptom_enteries = symp_df\n",
    "food_enteries = trig_df\n",
    "all_enteries = pd.concat([symp_df, trig_df], sort=False)\n",
    "all_enteries['day'] = pd.to_datetime(all_enteries['day'])\n",
    "\n",
    "\n",
    "display(all_enteries)\n",
    "#DF display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER STATS Function\n",
    "*not sure if this should be a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# USER USE STATS\n",
    "###########\n",
    "def userUseStats():\n",
    "\n",
    "\n",
    "    total_food_enteries = len(trig_df)\n",
    "\n",
    "    total_symptom_enteries = len(symp_df)\n",
    "\n",
    "    total_enteries = len(all_enteries)\n",
    "\n",
    "    number_days_with_enteries = len(all_enteries.day.unique())\n",
    "\n",
    "    first_entery = min(all_enteries['day'])\n",
    "\n",
    "    last_entry = max(all_enteries['day'])\n",
    "\n",
    "    return {\"total_food_enteries\": total_food_enteries, \"total_symptom_enteries\": total_symptom_enteries,\n",
    "            \"number_days_with_enteries\": number_days_with_enteries, \"first_entery \": first_entery,\n",
    "            \"last_entry\": last_entry, \"total_enteries\": total_enteries}\n",
    "\n",
    "\n",
    "def printStats(stats=userUseStats()):\n",
    "    for i in stats.items():\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# RANK : CITIZEN SCIENTIST\n",
    "###################\n",
    "# formula: number of enteries\n",
    "# Newbie\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rankBadge(stats=userUseStats()):\n",
    "\n",
    "    use_stats = stats\n",
    "    total_enteries = use_stats.get('total_enteries')\n",
    "    if total_enteries >= 10 and total_enteries < 15:\n",
    "        return 1\n",
    "    # Initiate Research\n",
    "    elif total_enteries >= 15 and total_enteries < 25:\n",
    "        return 2\n",
    "    # Casual Researcher\n",
    "    elif  total_enteries >= 25 and total_enteries < 50:\n",
    "        return 3\n",
    "    # Research Enthusiast\n",
    "    elif  total_enteries >= 50 and total_enteries < 100:\n",
    "        return 4\n",
    "    # Scholar\n",
    "    elif  total_enteries >= 100 and total_enteries < 200:\n",
    "        return 5\n",
    "    # Master Researcher\n",
    "    elif  total_enteries >= 200:\n",
    "        return 6\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC BADGE FUNCTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##COMPLEX DIET\n",
    "\n",
    "def complexDietBadge():\n",
    "    #print(trig_df.head()) Just seeing if everything worked\n",
    "    complexDiet = trig_df['trigger'].nunique()\n",
    "\n",
    "    if complexDiet > 50:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ARCHIVIST BADGE\n",
    "def archivistBadge():\n",
    "    max_enteries_per_day = all_enteries.groupby(['day'])[['symptom','trigger']].count()\n",
    "\n",
    "    max_enteries_per_day ## see data frame\n",
    "    ##create sum_symp_trig column out of symptom and trigger column\n",
    "    max_enteries_per_day['sum_symp_trig'] = max_enteries_per_day['symptom'] + max_enteries_per_day['trigger']\n",
    "    \n",
    "    #extract the maximum value as an int from sum_symp_trig\n",
    "    result = max_enteries_per_day['sum_symp_trig'].max().item()\n",
    "    \n",
    "    if result > 20:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##STREAK BADGE  ##BROKE ATM\n",
    "\"\"\"\n",
    "streak_enteries = all_enteries\n",
    "streak_enteries['streak_id'] = (streak_enteries['day']-streak_enteries['day'].shift() != 1).cumsum()\n",
    "\n",
    "streak_enteries.groupby('streak_id').count()\n",
    "\"\"\"\n",
    "##Coffee Badge\n",
    "\n",
    "def coffeeBadge():\n",
    "    coffee_count = trig_df['trigger'].str.contains('Coffee').sum() + trig_df['trigger'].str.contains('coffee').sum()#or any()\n",
    "    ##\n",
    "    ##TOP 10 COFFEE WORDS  :>OBJECT that is food :>Dairy, Gluten, \n",
    "    ##make variables which are top 10 of each food category \n",
    "    ##\n",
    "    #test_count = (trig_df['trigger'].str.contains('Coffee')) & (trig_df['trigger'].str.contains('coffee')).sum()\n",
    "\n",
    "    if coffee_count > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "coffeeBadge()\n",
    "    \n",
    "###WEEK END WARRIOR BADGE \n",
    "#BROKEN\n",
    "#https://stackoverflow.com/questions/30405413/python-pandas-extract-year-from-datetime-dfyear-dfdate-year-is-not\n",
    "    \n",
    "# day_of_the_week = all_enteries    \n",
    "# day_of_the_week['day_of_the_week'] = day_of_the_week['day'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printStats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rankBadge())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC badges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Complex diet\n",
    "print('Complex Diet')\n",
    "print(complexDietBadge())\n",
    "\n",
    "print('##Coffee Badge####################')\n",
    "print(coffeeBadge())\n",
    "\n",
    "print('ARCHIVIST')\n",
    "print(archivistBadge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = userUseStats()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    \"PK_ID\": [1],\n",
    "    \"User_ID\" : [\"Asaf\"], \n",
    "    \"tot_symp_stat\": stats.get('total_symptom_enteries'),  \n",
    "    \"tot_trig_stat\": stats.get('total_food_enteries'),\n",
    "    \"tot_entery_stat\": stats.get('total_enteries'),\n",
    "    \"first_entery\":  stats.get('first_entery'),\n",
    "    \"last_entery\" : stats.get('last_entry'),\n",
    "    \"rank_badge\" :  rankBadge(),\n",
    "    \"complex_diet_badge\" : complexDietBadge(),\n",
    "    \"archivist_badge\" : archivistBadge(),\n",
    "    \"coffee_badge\" : coffeeBadge()},\n",
    "    index = [1]\n",
    ")\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVER FLOW DISREGARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OVERFLOW DRAFT AREA###\n",
    "print('Stats#########################')\n",
    "printStats()\n",
    "print('RANK BADGE#########################')\n",
    "print(rankBadge())\n",
    "print('Complex DIET#########################')\n",
    "\n",
    "print(complexDietBadge())\n",
    "\n",
    "print('##Coffee Badge####################')\n",
    "print(coffeeBadge())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THE ARCHIVIST function doesn't work because I can't cast the dataframe into a number\n",
    "\n",
    "def archivist_badge():\n",
    "    \n",
    "    \n",
    "    if max_enteries_per_day.apply(pd.to_numeric) > 20:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "archivist_badge()\n",
    "\"\"\"\n",
    "\n",
    "df2 = pd.DataFrame(np.array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(coffee_count.trigger == \"coffee\" )|(coffee_count.trigger == latte )|\n"
     ]
    }
   ],
   "source": [
    "####ORIGIONAL\n",
    "#category\n",
    "base = r'{}'\n",
    "expr = '(?=.*{})'\n",
    "words = ['Coke', 'Coffee', 'cat']  # example\n",
    "searchString = base.format(''.join(expr.format(w) for w in words))\n",
    "\n",
    "#######NEW\n",
    "#category\n",
    "base = r'{}'\n",
    "expr = '(coffee_count.trigger == {} )|'\n",
    "words = [\n",
    "\n",
    "'\"coffee\"',\n",
    "'latte',\n",
    "\n",
    "\n",
    "\n",
    "]  # example\n",
    "searchString = base.format(''.join(expr.format(w) for w in words))\n",
    "\n",
    "\"\"\"\n",
    "'mocha',\n",
    "'americano',\n",
    "'cappuccino',\n",
    "'brappe',\n",
    "'breve',\n",
    "'expresso',\n",
    "'macchiato',\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(searchString)\n",
    "\n",
    "\n",
    "#coffee_count= len(coffee_count[(coffee_count.trigger == 'Coffee') | (coffee_count.trigger == 'coffee')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001E2F332BB38>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'coffee' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-46b1f1edf28f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m complex_coffee_count = len(\n\u001b[0;32m     23\u001b[0m             complex_coffee_count[\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mtrig_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcoffee\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrig_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrigger\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlatte\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             ])\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coffee' is not defined"
     ]
    }
   ],
   "source": [
    "##\n",
    "print(trig_df.groupby(by = trig_df['trigger']))\n",
    "\n",
    "\n",
    "###OLDER VERSION\n",
    "simple_coffee_count = trig_df\n",
    "simple_coffee_count= len(simple_coffee_count[(simple_coffee_count.trigger == 'Coffee') | (simple_coffee_count.trigger == 'coffee')])\n",
    "\n",
    "#isIN LIST FILTER\n",
    "filter = ['coffee','Coffee']\n",
    "coffee_count = trig_df\n",
    "\n",
    "result = coffee_count.trigger.isin(filter)-trig_df.count()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##COMPLEX\n",
    "complex_coffee_count = trig_df\n",
    "\n",
    "complex_coffee_count['trigger'] = complex_coffee_count['trigger'].str.lower()\n",
    "complex_coffee_count = len(\n",
    "            complex_coffee_count[\n",
    "            (trig_df.trigger == coffee )|(trig_df.trigger == latte )\n",
    "            ])\n",
    "\n",
    "\n",
    "display(complex_coffee_count)\n",
    "\n",
    "\n",
    "display(simple_coffee_count)\n",
    "##newer version\n",
    "test_coffee_count = (\"coffee\") in trig_df['trigger'].str.lower()\n",
    "\n",
    "##Newest version\n",
    "def coffee_check(x):\n",
    "    if x in (\"Coffee\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "Func = np.vectorize(coffee_check)\n",
    "\n",
    "c = trig_df.trigger\n",
    "\n",
    "print(\"VECTORIZE\")\n",
    "#print(c[Func(c)].count())\n",
    "\n",
    "print(\"GROUP BY\")\n",
    "print\n",
    "\n",
    "#print(test_coffee_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASS VERSION\n",
    "\n",
    "Consumes a userID and an optional data source.  If no data source is provided the data variable is a couple of csvs.  \n",
    "\n",
    "After the data is loaded, the get_user_stats method calculates user stats (e.g. number of food enteries, symptom enteries, total enteries, first entry, last entry).\n",
    "\n",
    "The userStats attribute is set using this method. \n",
    "\n",
    "\n",
    "The next section of the code calculates badges earned by the user. \n",
    "\n",
    "The get_rank_badge() method determines what level (rank) of badge the user has earned in the citizen scientist rank.  This badge correlates with the rank badge screen of the user stats area.  The rank is calculated by the number user enteries into the Dieta diary (e.g triggers, symptoms, etc.).  The more enteries the higher the rank. \n",
    "\n",
    "The get_complex_diet_badge() method returns a True/False value if the complex diet achievement has been earned.  The badge is dependent on the number of unique trigger items recorded.\n",
    "\n",
    "The get_archivist_badge method returns a True/False value to determine if the archivist achievement has been earned.  The method calculates the maximum number of diary enteries in a single day.  Users who hit a certain amount of enteries in a single day earn this badge. \n",
    "\n",
    "The get_coffee_badge returns a True/ False depending on the number of times a user has recorded coffee in the Dieta diary ( trigger colummn).  \n",
    "\n",
    "The next section deals with loading badges from an external badge table.  Some badges don't need to be recalculated.  The badge/ user stats area of the Dieta app can load pre calculated badges from the badge table. \n",
    "\n",
    "-----\n",
    "conjecture\n",
    "-----\n",
    "this method can consume \n",
    "\n",
    "\n",
    "The output section contains 3 methods.  The badge information can be output to the console (in a dataframe), a CSV, or to an external database.  \n",
    "\n",
    "The create_table() method will create a table from the values above.  \n",
    "The output_to_csv method will output the table to a csv file. \n",
    "The output_to_db method will output the table to a database. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Badge():\n",
    "    \n",
    "    def __init__(self, userID, csv, database=None )\n",
    "        self.userID = userID\n",
    "        \n",
    "        ##LOAD DATA##\n",
    "        #From Database\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        #From Data CSV for testing\n",
    "        else:\n",
    "            symp_df = pd.read_csv(sympPath)\n",
    "            trig_df = pd.read_csv(trigPath)\n",
    "            #symptom_enteries = symp_df\n",
    "            #food_enteries = trig_df\n",
    "            all_enteries = pd.concat([symp_df, trig_df], sort=False)\n",
    "            all_enteries['day'] = pd.to_datetime(all_enteries['day'])\n",
    "            all_enteries['ts'] = pd.to_datetime(all_enteries['ts']) #may break\n",
    "            \n",
    "            self.data = all_enteries\n",
    "    \n",
    "    \n",
    "    #stat function\n",
    "    ###########\n",
    "    # USER USE STATS\n",
    "    ###########\n",
    "    def get_user_stats(self):\n",
    "\n",
    "\n",
    "        total_food_enteries = len(all_enteries['trigger'])\n",
    "\n",
    "        total_symptom_enteries = len(all_enteries['symptom'])\n",
    "\n",
    "        total_enteries = len(all_enteries)\n",
    "\n",
    "        number_days_with_enteries = len(all_enteries.day.unique())\n",
    "\n",
    "        first_entery = min(all_enteries['day'])\n",
    "\n",
    "        last_entry = max(all_enteries['day'])\n",
    "\n",
    "        return {\"total_food_enteries\": total_food_enteries, \"total_symptom_enteries\": total_symptom_enteries,\n",
    "                \"number_days_with_enteries\": number_days_with_enteries, \"first_entery \": first_entery,\n",
    "                \"last_entry\": last_entry, \"total_enteries\": total_enteries}\n",
    "\n",
    "    ###SET USER STATS attribute\n",
    "    self.user_stats = get_user_stats():\n",
    "\n",
    "    \n",
    "    def display_user_stats(self):\n",
    "        for i in self.user_stats.items():\n",
    "            print(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #attribute badges\n",
    "    \n",
    "    #get badges\n",
    "    \n",
    "    ###################\n",
    "    # RANK : CITIZEN SCIENTIST\n",
    "    ###################\n",
    "    # formula: number of enteries\n",
    "    # Newbie\n",
    "\n",
    "\n",
    "    def get_rank_badge(stats=self.userStats):\n",
    "        \n",
    "        \n",
    "        use_stats = userStats\n",
    "        total_enteries = use_stats.get('total_enteries')\n",
    "        \n",
    "        #cut off values for RANK BADGES\n",
    "        newbVal = 10\n",
    "        initVal = 15\n",
    "        casVal = 25\n",
    "        enthusVal =50\n",
    "        scholVal =100\n",
    "        mastVal = 200\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #newbie\n",
    "        if total_enteries >= newbVal and total_enteries < initVal:\n",
    "            return 1\n",
    "        # Initiate Research\n",
    "        elif total_enteries >= initVal and total_enteries < casVal:\n",
    "            return 2\n",
    "        # Casual Researcher\n",
    "        elif  total_enteries >= casVal and total_enteries < enthusVal:\n",
    "            return 3\n",
    "        # Research Enthusiast\n",
    "        elif  total_enteries >= enthusVal and total_enteries < scholVal:\n",
    "            return 4\n",
    "        # Scholar\n",
    "        elif  total_enteries >= scholVal and total_enteries < mastVal:\n",
    "            return 5\n",
    "        # Master Researcher\n",
    "        elif  total_enteries >= mastVal:\n",
    "            return 6\n",
    "        else:\n",
    "            return 'broke'\n",
    "   \n",
    "    ############\n",
    "    #MISC BADGES\n",
    "    ###########\n",
    "    \n",
    "    ##COMPLEX DIET\n",
    "\n",
    "    def get_complex_diet_badge(self):\n",
    "        \n",
    "        COMPLEX_DIET_VAL = 50\n",
    "        \n",
    "        #print(trig_df.head()) Just seeing if everything worked\n",
    "        complexDiet = all_enteries['trigger'].count() #changed from nunique\n",
    "\n",
    "        if complexDiet > COMPLEX_DIET_VAL:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "    ##ARCHIVIST BADGE\n",
    "    def get_archivist_badge(self):\n",
    "        \n",
    "        ARCHIVIST_VAL = 20\n",
    "        \n",
    "        max_enteries_per_day = all_enteries.groupby(['day'])[['symptom','trigger']].count()\n",
    "\n",
    "        max_enteries_per_day ## see data frame\n",
    "        ##create sum_symp_trig column out of symptom and trigger column\n",
    "        max_enteries_per_day['sum_symp_trig'] = max_enteries_per_day['symptom'] + max_enteries_per_day['trigger']\n",
    "\n",
    "        #extract the maximum value as an int from sum_symp_trig\n",
    "        result = max_enteries_per_day['sum_symp_trig'].max().item()\n",
    "\n",
    "        if result > ARCHIVIST_VAL:\n",
    "            return True\n",
    "        else:\n",
    "            return False  \n",
    "        \n",
    "        \n",
    "    ##Coffee Badge\n",
    "\n",
    "    def get_coffee_badge(self):\n",
    "        \n",
    "        coffee_count = all_enteries['trigger'].str.contains('Coffee').sum() + all_enteries['trigger'].str.contains('coffee').sum()#or any()\n",
    "        ##\n",
    "        ##TOP 10 COFFEE WORDS  :>OBJECT that is food :>Dairy, Gluten, \n",
    "        ##make variables which are top 10 of each food category \n",
    "        ##\n",
    "        #test_count = (trig_df['trigger'].str.contains('Coffee')) & (trig_df['trigger'].str.contains('coffee')).sum()\n",
    "\n",
    "        if coffee_count > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ############\n",
    "    #load badges from DB\n",
    "    ############\n",
    "    def get_badges_from_table(self, badgeTableConnection):\n",
    "        print('This will display all the badges from a table')\n",
    "    \n",
    "    \n",
    "    ##########\n",
    "    #output \n",
    "    ##########\n",
    "    \n",
    "    #To Data Frame Method\n",
    "    def create_table():\n",
    "        stats = self.userStats\n",
    "\n",
    "        table = pd.DataFrame({\n",
    "            \"PK_ID\": [1],\n",
    "            \"User_ID\" : [\"Asaf\"], \n",
    "            \"tot_symp_stat\": stats.get('total_symptom_enteries'),  \n",
    "            \"tot_trig_stat\": stats.get('total_food_enteries'),\n",
    "            \"tot_entery_stat\": stats.get('total_enteries'),\n",
    "            \"first_entery\":  stats.get('first_entery'),\n",
    "            \"last_entery\" : stats.get('last_entry'),\n",
    "            \"rank_badge\" :  rankBadge(),\n",
    "            \"complex_diet_badge\" : complexDietBadge(),\n",
    "            \"archivist_badge\" : archivistBadge(),\n",
    "            \"coffee_badge\" : coffeeBadge()},\n",
    "            index = [1]\n",
    "        )\n",
    "    \n",
    "    def output_to_csv:\n",
    "        table = create_table()\n",
    "        table.to_csv(r'C:\\webDev\\pycharm\\dieta\\data\\export_badges.csv') \n",
    "    #https://datatofish.com/export-dataframe-to-csv/\n",
    "        \n",
    "    \n",
    "    #To Database output\n",
    "    def output_to_db():\n",
    "        table = create_table()\n",
    "        #Add to database here\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "    \n",
    "    #Not written\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#LOAD DATA\n",
    "\n",
    "sympPath =  Path('C:/webDev/pycharm/dieta/data/ak_symptoms.csv')\n",
    "trigPath =  Path('C:/webDev/pycharm/dieta/data/ak_triggers.csv')\n",
    "\n",
    "\n",
    "def prepare_csv():\n",
    "    symp_df = pd.read_csv(sympPath)\n",
    "    trig_df = pd.read_csv(trigPath)\n",
    "    #symptom_enteries = symp_df\n",
    "    #food_enteries = trig_df\n",
    "    all_enteries = pd.concat([symp_df, trig_df], sort=False)\n",
    "    all_enteries['day'] = pd.to_datetime(all_enteries['day'])\n",
    "    all_enteries['ts'] = pd.to_datetime(all_enteries['ts']) #may break\n",
    "\n",
    "asaf = Badge(1)\n",
    "print(asaf.get_user_stats())\n",
    "print(asaf.user_stats())\n",
    "\n",
    "print(asaf.get_rank_badge())\n",
    "\n",
    "print(get_complex_diet_badge)\n",
    "print(get_archivist_badge)\n",
    "print(get_coffee_badge)\n",
    "print(create_table())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
